{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Collision Avoidance - Data Collection\n",
    "\n",
    "If you ran through the basic motion notebook, hopefully you're enjoying how easy it can be to make your Jetbot move around! Thats very cool!  But what's even cooler, is making JetBot move around all by itself!  \n",
    "\n",
    "This is a super hard task, that has many different approaches but the whole problem is usually broken down into easier sub-problems.  It could be argued that one of the most\n",
    "important sub-problems to solve, is the problem of preventing the robot from entering dangerous situations!  We're calling this *collision avoidance*. \n",
    "\n",
    "In this set of notebooks, we're going to attempt to solve the problem using deep learning and a single, very versatile, sensor: the camera.  You'll see how with a neural network, camera, and the NVIDIA Jetson Nano, we can teach the robot a very useful behavior!\n",
    "\n",
    "The approach we take to avoiding collisions is to create a virtual \"safety bubble\" around the robot.  Within this safety bubble, the robot is able to spin in a circle without hitting any objects (or other dangerous situations like falling off a ledge).  \n",
    "\n",
    "\n",
    "Of course, the robot is limited by what's in it's field of vision, and we can't prevent objects from being placed behind the robot, etc.  But we can prevent the robot from entering these scenarios itself.\n",
    "\n",
    "The way we'll do this is super simple:  \n",
    "\n",
    "First, we'll manually place the robot in scenarios where it's \"safety bubble\" is violated, and label these scenarios ``blocked``.  We save a snapshot of what the robot sees along with this label.\n",
    "\n",
    "Second, we'll manually place the robot in scenarios where it's safe to move forward a bit, and label these scenarios ``free``.  Likewise, we save a snapshot along with this label.\n",
    "\n",
    "That's all that we'll do in this notebook; data collection.  Once we have lots of images and labels, we'll upload this data to a GPU enabled machine where we'll *train* a neural network to predict whether the robot's safety bubble is being violated based off of the image it sees.  We'll use this to implement a simple collision avoidance behavior in the end :)\n",
    "\n",
    "> IMPORTANT NOTE:  When JetBot spins in place, it actually spins about the center between the two wheels, not the center of the robot chassis itself.  This is an important detail to remember when you're trying to estimate whether the robot's safety bubble is violated or not.  But don't worry, you don't have to be exact. If in doubt it's better to lean on the cautious side (a big safety bubble).  We want to make sure JetBot doesn't enter a scenario that it couldn't get out of by turning in place.\n",
    "\n",
    "＃衝突回避-データ収集\n",
    "\n",
    "基本的なモーションノートブックを実行した場合は、Jetbotを簡単に移動できることをお楽しみください！それはとてもクールです！さらにクールなのは、JetBotを単独で移動させることです！\n",
    "\n",
    "これは非常に難しいタスクであり、さまざまなアプローチがありますが、通常、問題全体はより簡単なサブ問題に分解されます。最も多くの\n",
    "解決すべき重要な副次的な問題は、ロボットが危険な状況に入るのを防ぐ問題です！これを*衝突回避*と呼んでいます。\n",
    "\n",
    "このノートブックのセットでは、ディープラーニングと単一の非常に用途の広いセンサーであるカメラを使用して、問題の解決を試みます。ニューラルネットワーク、カメラ、およびNVIDIA Jetson Nanoを使用して、ロボットに非常に有用な動作を教えることができる方法がわかります。\n",
    "\n",
    "衝突を回避するためのアプローチは、ロボットの周りに仮想の「安全バブル」を作成することです。この安全バブル内で、ロボットは物体にぶつかることなく（または棚から落ちるなどのその他の危険な状況で）円を描くように回転できます。\n",
    "\n",
    "\n",
    "もちろん、ロボットはその視野内にあるものによって制限されており、ロボットの背後などにオブジェクトが置かれるのを防ぐことはできません。しかし、ロボットがこれらのシナリオ自体に入るのを防ぐことはできます。\n",
    "\n",
    "これを行う方法は非常に簡単です。\n",
    "\n",
    "まず、ロボットを「安全バブル」に違反するシナリオに手動で配置し、これらのシナリオに「ブロック」というラベルを付けます。このラベルとともに、ロボットが見るもののスナップショットを保存します。\n",
    "\n",
    "第二に、少し前進しても安全なシナリオにロボットを手動で配置し、これらのシナリオに「無料」のラベルを付けます。同様に、このラベルとともにスナップショットを保存します。\n",
    "\n",
    "このノートブックで行うことはこれだけです。データ収集。たくさんの画像とラベルができたら、このデータをGPU対応マシンにアップロードし、そこでニューラルネットワークを*トレーニング*して、見た画像に基づいてロボットの安全バブルが侵害されて いるかどうかを予測します。これを使用して、最後に単純な衝突回避動作を実装します:)\n",
    "\n",
    ">重要な注意：JetBotが所定の位置に回転すると、実際にはロボットシャーシ自体の中心ではなく、2つの車輪の間の中心を中心に回転します。これは、ロボットの安全バブルに違反しているかどうかを推定しようとしているときに覚えておくべき重要な詳細です。しかし、心配しないでください、あなたは正確である必要はありません。疑わしい場合は、慎重な側面（大きな安全バブル）に頼る方が良いでしょう。私たちは、JetBotが所定の場所に曲がっても抜け出せないシナリオに入らないようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display live camera feed\n",
    "\n",
    "So let's get started.  First, let's initialize and display our camera like we did in the *teleoperation* notebook.  \n",
    "\n",
    "> Our neural network takes a 224x224 pixel image as input.  We'll set our camera to that size to minimize the filesize of our dataset (we've tested that it works for this task).\n",
    "> In some scenarios it may be better to collect data in a larger image size and downscale to the desired size later.\n",
    "###ライブカメラフィードを表示する\n",
    "\n",
    "それでは始めましょう。 まず、* teleoperation *ノートブックで行ったようにカメラを初期化して表示しましょう。\n",
    "\n",
    ">ニューラルネットワークは、入力として224x224ピクセルの画像を受け取ります。 カメラをそのサイズに設定して、データセットのファイルサイズを最小化します（このタスクで機能することをテストしました）。\n",
    ">シナリオによっては、より大きな画像サイズでデータを収集し、後で目的のサイズにダウンスケールする方が良い場合があります。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, next let's create a few directories where we'll store all our data.  We'll create a folder ``dataset`` that will contain two sub-folders ``free`` and ``blocked``, \n",
    "where we'll place the images for each scenario.\n",
    "素晴らしい、次にすべてのデータを保存するいくつかのディレクトリを作成しましょう。 各シナリオの画像を配置する2つのサブフォルダーを無料でブロックするフォルダーデータセットを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "blocked_dir = 'dataset/blocked'\n",
    "free_dir = 'dataset/free'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(free_dir)\n",
    "    os.makedirs(blocked_dir)\n",
    "except FileExistsError:\n",
    "    print('Directories not created because they already exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you refresh the Jupyter file browser on the left, you should now see those directories appear.  Next, let's create and display some buttons that we'll use to save snapshots\n",
    "for each class label.  We'll also add some text boxes that will display how many images of each category that we've collected so far. This is useful because we want to make\n",
    "sure we collect about as many ``free`` images as ``blocked`` images.  It also helps to know how many images we've collected overall.\n",
    "左側のJupyterファイルブラウザーを更新すると、これらのディレクトリが表示されます。 次に、スナップショットを保存するために使用するいくつかのボタンを作成して表示します\n",
    "クラスラベルごと。 また、これまでに収集した各カテゴリの画像の数を表示するテキストボックスを追加します。 これは便利です\n",
    "必ず「ブロックされた」画像と同じ数の「無料の」画像を収集してください。 また、全体で収集した画像の数を知るのにも役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33d3af78ad447e083362f62d2c3bdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=0, layout=Layout(height='64px', width='128px')), Button(button_style='success', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba6fdf25b2347ebb7b161eb80489ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=0, layout=Layout(height='64px', width='128px')), Button(button_style='danger', de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "free_button = widgets.Button(description='add free', button_style='success', layout=button_layout)\n",
    "blocked_button = widgets.Button(description='add blocked', button_style='danger', layout=button_layout)\n",
    "free_count = widgets.IntText(layout=button_layout, value=len(os.listdir(free_dir)))\n",
    "blocked_count = widgets.IntText(layout=button_layout, value=len(os.listdir(blocked_dir)))\n",
    "\n",
    "display(widgets.HBox([free_count, free_button]))\n",
    "display(widgets.HBox([blocked_count, blocked_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, these buttons wont do anything.  We have to attach functions to save images for each category to the buttons' ``on_click`` event.  We'll save the value\n",
    "of the ``Image`` widget (rather than the camera), because it's already in compressed JPEG format!\n",
    "\n",
    "To make sure we don't repeat any file names (even across different machines!) we'll use the ``uuid`` package in python, which defines the ``uuid1`` method to generate\n",
    "a unique identifier.  This unique identifier is generated from information like the current time and the machine address.\n",
    "左側のJupyterファイルブラウザーを更新すると、これらのディレクトリが表示されます。 次に、各クラスラベルのスナップショットを保存するために使用するいくつかのボタンを作成して表示します。 また、これまでに収集した各カテゴリの画像の数を表 示するテキストボックスを追加します。 これは、ブロックされた画像と同じ数の無料画像を確実に収集したいので便利です。 また、全体で収集した画像の数を知るのにも役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "def save_snapshot(directory):\n",
    "    image_path = os.path.join(directory, str(uuid1()) + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image.value)\n",
    "\n",
    "def save_free():\n",
    "    global free_dir, free_count\n",
    "    save_snapshot(free_dir)\n",
    "    free_count.value = len(os.listdir(free_dir))\n",
    "    \n",
    "def save_blocked():\n",
    "    global blocked_dir, blocked_count\n",
    "    save_snapshot(blocked_dir)\n",
    "    blocked_count.value = len(os.listdir(blocked_dir))\n",
    "    \n",
    "# attach the callbacks, we use a 'lambda' function to ignore the\n",
    "# parameter that the on_click event would provide to our function\n",
    "# because we don't need it.\n",
    "free_button.on_click(lambda x: save_free())\n",
    "blocked_button.on_click(lambda x: save_blocked())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the buttons above should save images to the ``free`` and ``blocked`` directories.  You can use the Jupyter Lab file browser to view these files!\n",
    "\n",
    "Now go ahead and collect some data \n",
    "\n",
    "1. Place the robot in a scenario where it's blocked and press ``add blocked``\n",
    "2. Place the robot in a scenario where it's free and press ``add free``\n",
    "3. Repeat 1, 2\n",
    "\n",
    "> REMINDER: You can move the widgets to new windows by right clicking the cell and clicking ``Create New View for Output``.  Or, you can just re-display them\n",
    "> together as we will below\n",
    "\n",
    "Here are some tips for labeling data\n",
    "\n",
    "1. Try different orientations\n",
    "2. Try different lighting\n",
    "3. Try varied object / collision types; walls, ledges, objects\n",
    "4. Try different textured floors / objects;  patterned, smooth, glass, etc.\n",
    "\n",
    "Ultimately, the more data we have of scenarios the robot will encounter in the real world, the better our collision avoidance behavior will be.  It's important\n",
    "to get *varied* data (as described by the above tips) and not just a lot of data, but you'll probably need at least 100 images of each class (that's not a science, just a helpful tip here).  But don't worry, it goes pretty fast once you get going :)\n",
    "\n",
    "すばらしいです！これで、上のボタンは画像を `` free``および `` blocked``ディレクトリに保存するはずです。 Jupyter Labファイルブラウザを使用して、これらのファイルを表示できます！\n",
    "\n",
    "さあ、データを集めましょう\n",
    "\n",
    "1.ロボットがブロックされているシナリオにロボットを置き、「ブロックされた追加」を押します\n",
    "2.ロボットを空いているシナリオに置き、「add free」を押します\n",
    "3. 1、2を繰り返します\n",
    "\n",
    ">リマインダー：セルを右クリックして「出力用の新しいビューを作成」をクリックすると、ウィジェットを新しいウィンドウに移動できます。または、それらを再表示することもできます\n",
    ">以下で一緒に\n",
    "\n",
    "データにラベルを付けるためのヒントを次に示します\n",
    "\n",
    "1.別の向きを試してください\n",
    "2.別の照明を試す\n",
    "3.さまざまなオブジェクト/衝突タイプを試してください。壁、棚、オブジェクト\n",
    "4.さまざまなテクスチャの床/オブジェクトを試してください。パターン、滑らか、ガラスなど\n",
    "\n",
    "最終的に、ロボットが現実の世界で遭遇するシナリオのデータが多いほど、衝突回避の挙動は良くなります。それは重要です\n",
    "（上記のヒントで説明した）*さまざまな*データを取得するには、大量のデータだけでなく、おそらく各クラスの少なくとも100個の画像が必要です（これは科学ではなく、ここで役立つヒントです）。しかし、心配しないでください、あなたが始めたらかな り速くなります:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41864a4e958400591753c9e622e359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af7427a26dc45fc9dbf5bd395cee60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=1, layout=Layout(height='64px', width='128px')), Button(button_style='success', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7964f887ba854a9a867565824459a532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=1, layout=Layout(height='64px', width='128px')), Button(button_style='danger', de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(image)\n",
    "display(widgets.HBox([free_count, free_button]))\n",
    "display(widgets.HBox([blocked_count, blocked_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's close the camera conneciton properly so that we can use the camera in the later notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "Once you've collected enough data, we'll need to copy that data to our GPU desktop or cloud machine for training.  First, we can call the following *terminal* command to compress\n",
    "our dataset folder into a single *zip* file.\n",
    "\n",
    "> The ! prefix indicates that we want to run the cell as a *shell* (or *terminal*) command.\n",
    "\n",
    "> The -r flag in the zip command below indicates *recursive* so that we include all nested files, the -q flag indicates *quiet* so that the zip command doesn't print any output\n",
    "＃＃ 次\n",
    "\n",
    "十分なデータを収集したら、トレーニングのためにそのデータをGPUデスクトップまたはクラウドマシンにコピーする必要があります。 まず、次の* terminal *コマンドを呼び出して圧縮します\n",
    "データセットフォルダを1つの* zip *ファイルにまとめます。\n",
    "\n",
    ">！ prefixは、セルを* shell *（または* terminal *）コマンドとして実行することを示します。\n",
    "\n",
    ">以下のzipコマンドの-rフラグは* recursive *を示すため、すべてのネストされたファイルが含まれます。-qフラグは* quiet *を示すため、zipコマンドは出力を出力しません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r -q dataset.zip dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a file named ``dataset.zip`` in the Jupyter Lab file browser.  You should download the zip file using the Jupyter Lab file browser by right clicking and selecting ``Download``.\n",
    "\n",
    "Next, we'll need to upload this data to our GPU desktop or cloud machine (we refer to this as the *host*) to train the collision avoidance neural network.  We'll assume that you've set up your training\n",
    "machine as described in the JetBot WiKi.  If you have, you can navigate to ``http://<host_ip_address>:8888`` to open up the Jupyter Lab environment running on the host.  The notebook you'll need to open there is called ``collision_avoidance/train_model.ipynb``.\n",
    "\n",
    "So head on over to your training machine and follow the instructions there!  Once your model is trained, we'll return to the robot Jupyter Lab enivornment to use the model for a live demo!\n",
    "Jupyter Labファイルブラウザーに `` dataset.zip``という名前のファイルが表示されます。 Jupyter Labファイルブラウザを使用して、右クリックして「ダウンロード」を選択し、zipファイルをダウンロードする必要があります。\n",
    "\n",
    "次に、このデータをGPUデスクトップまたはクラウドマシン（これを*ホスト*と呼びます）にアップロードして、衝突回避ニューラルネットワークをトレーニングする必要があります。 トレーニングをセットアップしたと仮定します\n",
    "JetBot WiKiで説明されているマシン。 お持ちの場合は、「http：// <host_ip_address>：8888」に移動して、ホストで実行されているJupyter Lab環境を開くことができます。 そこで開く必要があるノートブックは、「collision_avoidance / train_model.ipynb」と呼ばれます。\n",
    "\n",
    "だからあなたのトレーニングマシンに向かい、そこの指示に従ってください！ モデルがトレーニングされると、ロボットJupyter Labの環境に戻り、モデルをライブデモに使用します！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
